{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "Y1ps5BQSbz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "oWVm17uamQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "def data_generator(filepath, tokenizer, batch_size=32, max_length=100, test_size=0.2):\n",
        "    \"\"\"\n",
        "    A generator function that yields batches of tokenized and padded sequences and their labels.\n",
        "    \n",
        "    Parameters:\n",
        "    - filepath: Path to the JSONL file.\n",
        "    - tokenizer: An instance of tf.keras.preprocessing.text.Tokenizer.\n",
        "    - batch_size: The number of samples to return in each batch.\n",
        "    - max_length: The maximum length of the sequences after padding.\n",
        "    - test_size: The proportion of the dataset to include in the test split.\n",
        "    \n",
        "    Yields:\n",
        "    - A tuple (batch_sequences, batch_labels), where:\n",
        "        - batch_sequences is a numpy array of tokenized and padded sequences.\n",
        "        - batch_labels is a numpy array of labels for each sequence in the batch.\n",
        "    \"\"\"\n",
        "    titles = []\n",
        "    view_counts = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        for line in tqdm(file, desc=\"Loading and processing data\"):\n",
        "            record = json.loads(line)\n",
        "            titles.append(record['title'])\n",
        "            view_counts.append(record['view_count'])\n",
        "            \n",
        "            if len(titles) == batch_size:\n",
        "                sequences = tokenizer.texts_to_sequences(titles)\n",
        "                padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "                labels = np.log(np.array(view_counts, dtype=np.float32))\n",
        "                labels = np.where(labels == -np.inf, 0, labels)\n",
        "                \n",
        "                yield padded_sequences, labels\n",
        "                \n",
        "                titles = []\n",
        "                view_counts = []\n",
        "                \n",
        "    if titles:\n",
        "        sequences = tokenizer.texts_to_sequences(titles)\n",
        "        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "        labels = np.log(np.array(view_counts, dtype=np.float32))\n",
        "        labels = np.where(labels == -np.inf, 0, labels)\n",
        "        \n",
        "        yield padded_sequences, labels\n",
        "\n",
        "def sample_titles(filepath, sample_size=1000):\n",
        "    \"\"\"\n",
        "    Reads a sample of titles from a JSONL file.\n",
        "\n",
        "    Parameters:\n",
        "    - filepath: Path to the JSONL file.\n",
        "    - sample_size: Number of titles to sample.\n",
        "    \n",
        "    Returns:\n",
        "    - A list of sampled titles.\n",
        "    \"\"\"\n",
        "    titles = []\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if len(titles) >= sample_size:\n",
        "                break\n",
        "            record = json.loads(line)\n",
        "            titles.append(record['title'])\n",
        "    return titles\n",
        "\n",
        "\n",
        "max_length = 100\n",
        "filepath = '/mnt/datassd/processed_file.jsonl'\n",
        "\n",
        "titles_sample = sample_titles(filepath)\n",
        "\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "\n",
        "tokenizer.fit_on_texts(titles_sample)\n",
        "\n",
        "#tokenizer.fit_on_texts(titles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "UZ5jTpSq96"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import keras\n",
        "import keras_nlp\n",
        "from keras import layers\n",
        "\n",
        "vocab_size = 10000  # Adjust based on your vocabulary size\n",
        "embedding_dim = 256\n",
        "max_length = 100  # Adjust based on your titles' maximum length\n",
        "num_heads = 8  # Number of attention heads in the Transformer encoder\n",
        "intermediate_dim = 512  # Dimensionality of the encoder's intermediate (feed-forward) layer\n",
        "\n",
        "# Define input layer\n",
        "inputs = keras.Input(shape=(max_length,), dtype='int64')\n",
        "\n",
        "# Token and position embedding layer\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=vocab_size,\n",
        "    sequence_length=max_length,\n",
        "    embedding_dim=embedding_dim,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "# Transformer encoder layer\n",
        "encoder = keras_nlp.layers.TransformerEncoder(\n",
        "    num_heads=num_heads,\n",
        "    intermediate_dim=intermediate_dim,\n",
        "    activation='relu',\n",
        "    dropout=0.1,\n",
        ")\n",
        "x = encoder(x)\n",
        "\n",
        "# GlobalMaxPooling1D layer for regression task\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "# Additional dense layers\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "outputs = layers.Dense(1, activation='linear')(x)  # Linear activation for regression\n",
        "\n",
        "# Compile the model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "A5CwqIiqkS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# MLflow tracking\n",
        "mlflow.autolog()\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 1024  # This is now only for your reference and generator configuration\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Assuming your data_generator now correctly configures batches of size `batch_size`\n",
        "    model.fit(x=data_generator(filepath, tokenizer, batch_size, max_length),\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=83419,verbose = 0)  # Make sure this matches your actual number of batches per epoch)\n",
        "\n",
        "    # Log additional metrics or parameters\n",
        "    mlflow.log_param(\"vocab_size\", vocab_size)\n",
        "    mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
        "    mlflow.log_param(\"max_length\", max_length)\n",
        "    mlflow.log_param(\"num_heads\", num_heads)\n",
        "    mlflow.log_param(\"intermediate_dim\", intermediate_dim)\n",
        "    mlflow.log_param(\"epochs\", epochs)\n",
        "    mlflow.log_param(\"generator_batch_size\", batch_size)  # Renamed to clarify this is the generator's batch size\n",
        "    \n",
        "    # Save and log the model in MLflow\n",
        "    model_name = \"YT_Transformer\"\n",
        "    model_path = f\"{model_name}.keras\"\n",
        "    model.save(model_path)\n",
        "    mlflow.keras.log_model(model, \"model\", registered_model_name=model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "NjVou6EDo0"
      },
      "outputs": [],
      "source": [
        "#model.save(f'{actual_model_name}.keras')\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "for i in range(10):  # Display first 10 predictions\n",
        "    print(f\"Predicted view count: {predictions[i]}, Actual view count: {y_test[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "nNomoUvqzW"
      },
      "outputs": [],
      "source": [
        "# Make a line \n",
        "x = np.linspace(0,10,100)\n",
        "y = np.linspace(0,10,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "NWJd7aQ28G"
      },
      "outputs": [],
      "source": [
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "#\n",
        "#\n",
        "#heatmap, xedges, yedges = np.histogram2d(y_test.flatten(), predictions.flatten(), bins=100)\n",
        "#\n",
        "#extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
        "#\n",
        "#\n",
        "#plt.imshow(heatmap.T, extent=extent, origin='lower')\n",
        "#plt.plot(x,y, 'r--')\n",
        "#plt.xlabel('Views Order of Magnitude')\n",
        "#plt.ylabel('Predicted Order of Magnitude')\n",
        "#plt.xlim(2,15)\n",
        "#plt.ylim(0,17)\n",
        "#plt.savefig(f'{actual_model_name}_heatmap_bonito.png')\n",
        "#\n",
        "#import matplotlib.pyplot as plt\n",
        "#\n",
        "#\n",
        "#plt.scatter(y_test, predictions, alpha=0.1, s=0.5)\n",
        "#plt.plot(x,y,'r--')\n",
        "#plt.xlabel('Actual View Count')\n",
        "#plt.ylabel('Predicted View Count')\n",
        "#plt.savefig(f'{actual_model_name}_scatter_bonit.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "v61GjURvbq"
      },
      "outputs": [],
      "source": [
        "# If you need to convert an array of values\n",
        "y_test_e = np.exp(y_test)  # Assuming y_test was in loge form\n",
        "y_test_10 = np.log10(y_test_e)\n",
        "\n",
        "predictions_e = np.exp(predictions)  # Assuming predictions were in loge form\n",
        "predictions_10 = np.log10(predictions_e)\n",
        "\n",
        "y_test = y_test_10\n",
        "predictions = predictions_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "ohrretbeKu"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test and predictions are available and in log form\n",
        "# Heatmap\n",
        "heatmap, xedges, yedges = np.histogram2d(y_test.flatten(), predictions.flatten(), bins=100)\n",
        "extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.set(style=\"white\")\n",
        "\n",
        "# Using a colormap (e.g., 'viridis' which is visually appealing and colorblind-friendly)\n",
        "plt.imshow(heatmap.T, extent=extent, origin='lower', aspect='auto', cmap='viridis')\n",
        "\n",
        "# Assuming x, y for the red dashed line are defined correctly and correspond to log scale\n",
        "plt.plot(x, y, 'r--')\n",
        "\n",
        "plt.xlabel('Log of Actual View Count')\n",
        "plt.ylabel('Log of Predicted View Count')\n",
        "plt.colorbar(label='Count of Test')\n",
        "plt.title('Heatmap of Predictions vs Actual Views')\n",
        "plt.xlim(0, 9)\n",
        "plt.ylim(0, 9)\n",
        "\n",
        "# Adjusting x and y axis to show in 10^ format\n",
        "ax = plt.gca()\n",
        "ax.set_xticklabels([f'$10^{{{int(float(label))}}}$' for label in ax.get_xticks()])\n",
        "ax.set_yticklabels([f'$10^{{{int(float(label))}}}$' for label in ax.get_yticks()])\n",
        "\n",
        "plt.savefig(f'{actual_model_name}_heatmap_bonito.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "sXLwhEE9CR"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Scatter plot with adjustments for alpha and size for better visibility\n",
        "plt.scatter(y_test, predictions, alpha=0.2, s=10, cmap='viridis')\n",
        "\n",
        "plt.plot(x, y, 'r--')  # Assuming x, y for the red dashed line are correct\n",
        "\n",
        "plt.xlabel('Log of Actual View Count')\n",
        "plt.ylabel('Log of Predicted View Count')\n",
        "plt.title('Scatter Plot of Predicted vs Actual Views')\n",
        "plt.xlim(0, 9)\n",
        "plt.ylim(0, 9)\n",
        "\n",
        "\n",
        "# Adjust axis to reflect 10^x and 10^y\n",
        "ax = plt.gca()\n",
        "ax.set_xticklabels([f'$10^{{{int(float(label))}}}$' for label in ax.get_xticks()])\n",
        "ax.set_yticklabels([f'$10^{{{int(float(label))}}}$' for label in ax.get_yticks()])\n",
        "\n",
        "plt.savefig(f'{actual_model_name}_scatter_bonito.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jukit_cell_id": "9IRxPNoVbP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
