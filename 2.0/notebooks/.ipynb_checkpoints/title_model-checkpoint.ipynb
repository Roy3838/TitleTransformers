{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sys imports\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "# Training API\n",
    "import keras as keras\n",
    "from keras import layers\n",
    "import keras_nlp\n",
    "\n",
    "# Data manipulation and exploration\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, kurtosis\n",
    "\n",
    "# Data visualization\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# MLOps API\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "There are 72M titles and views in the data used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.dataloader import parse_jsonl_optimized\n",
    "\n",
    "# Example usage: Import only the first 1000 lines from the file\n",
    "file_path = '/mnt/datassd/train_data.jsonl'\n",
    "\n",
    "# Pass None if you want ALL lines\n",
    "num_lines_to_import = None \n",
    "\n",
    "titles, view_counts = parse_jsonl_optimized(file_path, num_lines_to_import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create $Log_{10}(Data)$ and create tokenizer\n",
    "\n",
    "When views are analyzed on their own, the MSE loss function does not make a lot of sense, there is a lot of variation in the data.\n",
    "\n",
    "What's why we take $log_{10}$ to get the order of magnitude of the views ($10^x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load BERT tokanizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding all 70M datapoints takes ~45mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SINGLE THREADED  \"\"\"\n",
    "encoded_inputs = [tokenizer.encode(title, add_special_tokens=True) for title in tqdm(titles, total=len(titles), desc=\"Encoding\")]\n",
    "del titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of title engagement, UPPER CASED titles correlate with engagement, it is important to use a CASED tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing cased inputs in tokenization\n",
    "titulos_raros = [r'IS THIS TOKEN Cased?']\n",
    "print(tokenizer.tokenize(titulos_raros[0]))\n",
    "tokenizer.encode(titulos_raros[0], add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_view_count = np.where(np.log10(view_counts) == -np.inf, 0, np.log10(view_counts))\n",
    "\n",
    "# Bin the view counts in ranges 0-1, 1-2, 2-3, 3-4, 4-5, 5-6, 6-7, 7-8, 8+\n",
    "def bin_data(views, a, b, n_bins):\n",
    "    # Generate bin edges from a to b with n_bins\n",
    "    if b > a and n_bins > 1:\n",
    "        bin_edges = np.linspace(a, b, n_bins - 1)\n",
    "    else:\n",
    "        raise ValueError(\"Ensure that b > a and n_bins > 1\")\n",
    "    \n",
    "    # Bin the data into categories\n",
    "    view_bins = np.digitize(views, bin_edges, right=True)\n",
    "\n",
    "    # Categorize data into n_bins categories\n",
    "    view_categorical = to_categorical(view_bins, num_classes=n_bins)\n",
    "\n",
    "    return view_categorical, view_bins\n",
    "\n",
    "\n",
    "def reverse_bin_data(categorical_views, a, b, n_bins):\n",
    "    # Check for valid input\n",
    "    if b <= a or n_bins <= 1:\n",
    "        raise ValueError(\"Ensure that b > a and n_bins > 1\")\n",
    "\n",
    "    # Calculate the bin width\n",
    "    bin_width = (b - a) / n_bins\n",
    "    \n",
    "    # Find the bin indices from the categorical data\n",
    "    bin_indices = np.argmax(categorical_views, axis=1)\n",
    "    \n",
    "    # Calculate the representative value for each bin. Using the bin center as the representative value.\n",
    "    representative_values = a + bin_width * (bin_indices + 0.5)\n",
    "\n",
    "    return representative_values\n",
    "\n",
    "# IMPORTANT, IT SETS THE SCALE\n",
    "a = 0\n",
    "b = 8\n",
    "n_bins = 16\n",
    "\n",
    "\n",
    "view_categorical, view_bins = bin_data(log_view_count, a, b, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_view_count[10], reverse_bin_data(view_categorical, a, b, n_bins)[10],view_categorical[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization tests\n",
    "\n",
    "First, let's see the distribution of view counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Y view count distribution\n",
    "\n",
    "\n",
    "sns.histplot(log_view_count, kde=True)\n",
    "plt.title(\"Log of View Count Distribution\")\n",
    "plt.xlabel(\"Log of View Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test_stat, shapiro_p_value = shapiro(log_view_count)\n",
    "kurtosis_value = kurtosis(log_view_count, fisher=True)\n",
    "\n",
    "shapiro_test_stat, shapiro_p_value, kurtosis_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_titles_i = 500\n",
    "print(view_counts[sample_titles_i:sample_titles_i+5])\n",
    "print(log_view_count[sample_titles_i:sample_titles_i+5])\n",
    "titles[sample_titles_i:sample_titles_i+5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histogram the bins\n",
    "sns.histplot(view_bins, kde=False)\n",
    "plt.title(\"View Count Bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the ditribution of lenghts of titles.\n",
    "\n",
    "We need to have a cutoff at a certain token lenght. So let's visualize when it would be appropriate to cut titles off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot([len(encoded_input) for encoded_input in encoded_inputs], bins=50)\n",
    "plt.title(\"Histogram of tokenized title lengths\")\n",
    "plt.xlabel(\"Length of tokenized title\")\n",
    "plt.ylabel(\"Number of titles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably at 40 tokens, the cutoff would preserve most of the information. (Remember YouTube titles have a 100 char limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40  # Ensure your data is adjusted accordingly\n",
    "\n",
    "padded_inputs = pad_sequences(encoded_inputs, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making sure, all inputs are the same length of 40 \n",
    "sns.histplot([len(padded_input) for padded_input in padded_inputs], bins=50)\n",
    "plt.title(\"Histogram of padded title lengths\")\n",
    "plt.xlabel(\"Length of padded title\")\n",
    "plt.ylabel(\"Number of titles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some samples to see the PADs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [tokenizer.decode(padded_input) for padded_input in padded_inputs[90:100]]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = padded_inputs\n",
    "del padded_inputs\n",
    "Y_t = view_categorical\n",
    "del view_categorical\n",
    "\n",
    "vocab_size = 30522  # Adjusted to match BERT's vocabulary size for bert-base-cased\n",
    "embedding_dim = 768  # Standard BERT-base embedding dimension\n",
    "transformer_encoder_layers = 1  # Increase this for a larger model\n",
    "num_heads = 12  # Increase this for a larger model\n",
    "intermediate_dim = 768*2  # Dimensionality of the encoder layers and the pooler layer\n",
    "dropout_rate = 0.1  # Dropout rate for regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "inputs = keras.Input(shape=(max_length,), dtype='int32')\n",
    "\n",
    "# Embedding layer\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=vocab_size, \n",
    "    sequence_length=max_length, \n",
    "    embedding_dim=embedding_dim,\n",
    ")\n",
    "\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "# Transformer blocks\n",
    "for _ in range(transformer_encoder_layers):\n",
    "    encoder = keras_nlp.layers.TransformerEncoder(\n",
    "        num_heads=num_heads,\n",
    "        intermediate_dim=intermediate_dim,\n",
    "        activation='relu',\n",
    "        dropout=dropout_rate,\n",
    "    )\n",
    "    x = encoder(x)\n",
    "\n",
    "# Pooling and output layers\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(embedding_dim, activation='relu')(x)\n",
    "outputs = layers.Dense(n_bins, activation='softmax')(x)  # 8 classes for the bins\n",
    "\n",
    "# Model compilation\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape, Y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def batch_loader(X, Y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)  # Shuffle if you want to randomize the batches\n",
    "    \n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], Y[batch_indices]\n",
    "\n",
    "class MlflowCallbackLogPerBatch(mlflow.keras_core.MLflowCallback):\n",
    "    def __init__(self, run, log_every_n_steps):\n",
    "        super().__init__(run=run)\n",
    "        self.log_every_n_steps = log_every_n_steps\n",
    "        self._log_step = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "        if (batch + 1) % self.log_every_n_steps == 0:\n",
    "            # Extract scalar values from the numpy arrays\n",
    "            loss = logs['loss'][0].item()  # Convert numpy array to Python scalar\n",
    "            accuracy = logs['loss'][1].item()  # Convert numpy array to Python scalar\n",
    "            \n",
    "            # Log the metrics using MLflow\n",
    "            mlflow.log_metrics({'loss': loss, 'accuracy': accuracy}, step=self._log_step)\n",
    "            self._log_step += 1\n",
    "\n",
    "\n",
    "\n",
    "# Start MLflow run and train the model\n",
    "with mlflow.start_run() as run:\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    batch_size = 32\n",
    "    epochs = 5\n",
    "    callback = MlflowCallbackLogPerBatch(run=run, log_every_n_steps=5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        # Setup tqdm for the batch loop\n",
    "        with tqdm(total=len(X_t) // batch_size) as pbar:\n",
    "            for batch, (X_batch, Y_batch) in enumerate(batch_loader(X_t, Y_t, batch_size)):\n",
    "                history = model.train_on_batch(X_batch, Y_batch)\n",
    "                logs = {'loss': history}\n",
    "                callback.on_batch_end(batch=batch, logs=logs)\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "        # Optionally validate your model here if needed\n",
    "\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eval input\n",
    "eval_file_path = '/mnt/datassd/train_data.jsonl'\n",
    "titles_eval, view_counts_eval = parse_jsonl_optimized(eval_file_path, num_lines_to_import*0.005)\n",
    "encoded_evals = [tokenizer.encode(title_ev, add_special_tokens=True) for title_ev in tqdm(titles_eval, total=len(titles_eval), desc=\"Encoding\")]\n",
    "padded_evals = pad_sequences(encoded_evals, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# eval output\n",
    "log_view_count_eval = np.where(np.log10(view_counts_eval) == -np.inf, 0, np.log10(view_counts_eval))\n",
    "view_evals_test, view_evals_bins = bin_data(log_view_count_eval, a, b, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_e = view_evals_test\n",
    "X_e = padded_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_e)\n",
    "actual = log_view_count_eval\n",
    "predicted = reverse_bin_data(Y_pred, a, b, n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# actual = []\n",
    "# predicted = []\n",
    "\n",
    "# for i in range(len(Y_pred)):\n",
    "#     pr = reverse_bin_data(Y_pred[i], a, b, n_bins)\n",
    "#     ac = reverse_bin_data(Y_e[i], a, b, n_bins)\n",
    "#     print(f\"Predicted: {pr}, Real: {ac}\")\n",
    "#     actual.append(ac)\n",
    "#     predicted.append(pr)\n",
    "\n",
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scatter of predicted vs actual\n",
    "plt.scatter(actual, predicted,alpha=0.005, s=10, cmap='viridis')\n",
    "# Make a line\n",
    "plt.plot([0, 10], [0, 10], color='red')\n",
    "plt.title(\"Predicted vs Actual Score\")\n",
    "plt.xlabel(\"Actual Score View Count\")\n",
    "plt.ylabel(\"Predicted Score View Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(predicted, kde=False)\n",
    "plt.title(\"Predicted Score View Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arange titles by highest predicted view count\n",
    "sorted_titles = [title for _, title in sorted(zip(predicted, titles_eval), reverse=True)]\n",
    "sorted_titles[:50], sorted_titles[-50:]\n",
    "\n",
    "# corresponding view counts\n",
    "sorted_view_counts = [view_count for _, view_count in sorted(zip(predicted, view_counts_eval), reverse=True)]\n",
    "\n",
    "\n",
    "# Do both in one line\n",
    "sorted_titles, sorted_view_counts = zip(*[(title, view_count) for _, title, view_count in sorted(zip(predicted, titles_eval, view_counts_eval), reverse=True, key=lambda x: x[0])])\n",
    "sorted_titles[:5], sorted_titles[-5:], sorted_view_counts[:5], sorted_view_counts[-5:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
